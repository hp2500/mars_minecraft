---
title: "Process Data Analysis"
author: "Heinrich Peters"
date: "4/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# MAC
knitr::opts_knit$set(root.dir = '/Users/hp2500/Google Drive/STUDY/Columbia/Research/Minecraft/Data')

```



# Import Libraries
```{r, results = "hide", message=FALSE}

library(tidyverse)
library(dplyr)
library(tidyr)
library(caret)
library(glmnet)

```

# Import Data
```{r, results = "hide", message=FALSE}

pc <- read.csv('pc.csv')
mr <- read.csv('mr.csv')
sc <- read.csv('sc.csv')
df_all <- read.csv('df_all.csv')

```

# Analyze Process Data 



## Exctract Process Features - SC
```{r}

variables = c('id', 'item', 'time_taken', 'distance_travelled', 
         'xPos', 'yPos', 'zPos', 'Pitch', 'Yaw', 'ray.x.', 'ray.y.', 
         'ray.z.', 'ray.distance.', 'distance_goal', 'looking_model', 
         'looking_goal', 'steps', 'correctBlocks', 'incorrectBlocks', 
         'answer')

sc_feat <- sc %>% 
  #mutate(id = as.factor(id)) %>%
  dplyr::select(all_of(variables)) %>% 
  #filter(item != 1 & item != 2) %>%
  dplyr::group_by(id) %>%
  dplyr::summarize(distance_travelled = max(as.numeric(distance_travelled), na.rm = T),
            xPos_range = diff(range(as.numeric(xPos), na.rm = T)), 
            xPos_mean = mean(as.numeric(xPos), na.rm = T), 
            yPos_range = diff(range(as.numeric(yPos), na.rm = T)), 
            yPos_mean = mean(as.numeric(yPos), na.rm = T), 
            zPos_range = diff(range(as.numeric(zPos), na.rm = T)),
            zPos_mean = mean(as.numeric(zPos), na.rm = T), 
            pitch_range = diff(range(as.numeric(Pitch), na.rm = T)),
            pitch_mean = mean(as.numeric(Pitch), na.rm = T), 
            yaw_range = diff(range(as.numeric(Yaw), na.rm = T)),
            yaw_mean = mean(as.numeric(Yaw), na.rm = T),
            ray_x_range = diff(range(as.numeric(ray.x.), na.rm = T)),
            ray_x_mean = mean(as.numeric(ray.x.), na.rm = T),
            ray_y_range = diff(range(as.numeric(ray.y.), na.rm = T)),
            ray_z_mean = mean(as.numeric(ray.z.), na.rm = T),
            ray_dist_range = diff(range(as.numeric(ray.distance.), na.rm = T)),
            ray_dist_mean = mean(as.numeric(ray.distance.), na.rm = T), 
            looking_model = max(as.numeric(looking_model), na.rm = T),
            looking_goal = max(as.numeric(looking_goal), na.rm = T),
            looking_ratio = looking_model/looking_goal) %>%
  ungroup()

dim(sc_feat)

 
sc_feat_item <- sc %>% 
  dplyr::select(all_of(variables)) %>% 
  filter(item != 1 & item != 2) %>%
  dplyr::group_by(id, item) %>%
  dplyr::summarise(distance_travelled = max(as.numeric(distance_travelled), na.rm = T),
            xPos_range = diff(range(as.numeric(xPos), na.rm = T)), 
            xPos_mean = mean(as.numeric(xPos), na.rm = T), 
            yPos_range = diff(range(as.numeric(yPos), na.rm = T)), 
            yPos_mean = mean(as.numeric(yPos), na.rm = T), 
            zPos_range = diff(range(as.numeric(zPos), na.rm = T)),
            zPos_mean = mean(as.numeric(zPos), na.rm = T), 
            pitch_range = diff(range(as.numeric(Pitch), na.rm = T)),
            pitch_mean = mean(as.numeric(Pitch), na.rm = T), 
            yaw_range = diff(range(as.numeric(Yaw), na.rm = T)),
            yaw_mean = mean(as.numeric(Yaw), na.rm = T),
            ray_x_range = diff(range(as.numeric(ray.x.), na.rm = T)),
            ray_x_mean = mean(as.numeric(ray.x.), na.rm = T),
            ray_y_range = diff(range(as.numeric(ray.y.), na.rm = T)),
            ray_z_mean = mean(as.numeric(ray.z.), na.rm = T),
            ray_dist_range = diff(range(as.numeric(ray.distance.), na.rm = T)),
            ray_dist_mean = mean(as.numeric(ray.distance.), na.rm = T), 
            looking_model = max(as.numeric(looking_model), na.rm = T),
            looking_goal = max(as.numeric(looking_goal), na.rm = T),
            looking_ratio = looking_model/looking_goal) %>% 
  ungroup()

dim(sc_feat_item)


# spread variables by item 
sc_feat_item_wide = sc_feat_item %>% dplyr::select(id) %>% unique()

for (i in names(sc_feat_item)[3:length(sc_feat_item)]){
  
  long_temp <- sc_feat_item %>% dplyr::select(id, item, all_of(i)) 
  wide_temp <- long_temp %>% tidyr::spread(key=item, value = i) 
  names(wide_temp) <- paste("sc",names(wide_temp), i, sep='_')
  names(wide_temp)[1] <- "id"
  sc_feat_item_wide = sc_feat_item_wide %>% inner_join(wide_temp, by="id")
}


dim(sc_feat_item_wide)


# sc_feat_item_wide %>% is.na() %>% colSums()


```

## Merge data
```{r}

sc_feat_all <- sc_feat %>% inner_join(df_all, by="id") 
sc_feat_all_item_wide <- sc_feat_item_wide %>% inner_join(df_all, by="id") 



```

## Exploratory analyses
```{r}

# correlation table 
sc_feat_all_cor <- sc_feat_all %>% dplyr::select(-id) %>% 
  cor() %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  filter(rowname %in% c("pc", "mr", "sc", "VKMR", "SPM"))

sc_feat_all_cor


# correlation table 
sc_feat_all_item_wide_cor <- sc_feat_all_item_wide %>% dplyr::select(-id) %>% 
  cor(use="complete.obs") %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  filter(rowname %in% c("pc", "mr", "sc", "VKMR", "SPM"))

sc_feat_all_item_wide_cor




```



## Build function for nested CV
```{r}

nested_cv <- function(data, target, inner_method = 'cv', inner_number = 10,
                      outer_method = 'kfold', outer_number = 10){
  
            
  # cross validation strategy for inner loop
  ctrl <- trainControl(method = "cv", number = 10)
  
  # parameter grid for elastic net
  enet_grid <- expand.grid(.lambda = seq(.05, 1, length = 10),
                      .alpha = seq(.05, 1, length = 10))
  
  # parameter grid for random forest
  rf_grid <- data.frame(.mtry = 2:(ncol(training) - 1L))
  
  # parameter grid for gradient boosting
  gbm_grid <- expand.grid(.interaction.depth = seq(1, 5, by = 1),
                        .n.trees = seq(100, 1000, by = 100),
                        .shrinkage = c(0.01, 0.1),
                        .n.minobsinnode = 1:10)
  
  
  # create empty list for results
  cv_results <- list('enet' = NULL, 'rf' = NULL, 'gb' = NULL)
  
  # create folds for outer k-fold cv
  kfolds <- rep(1:outer_number, length.out = nrow(data)) %>% sample(replace = F)

  # nested cross validation loop 
  for (i in 1:outer_number){
    
    print(paste('Outer loop:', i))
    
    # train test split outer loop
    if (outer_method == 'monte_carlo'){
    in_train <- createDataPartition(y = data[[target]],
                                  p = 0.9, list = FALSE)
    }
    
    if (outer_method == 'kfold'){
    in_train <- which(kfolds != i)
    }
  
    training <- data[ in_train, ]
    testing  <- data[-in_train, ]
    
    X_train <- training %>% select(-target)
    y_train <- training[[target]] 
    
    X_test <- testing %>% select(-target)
    y_test <- testing[[target]]
  
    
    #### LASSO
    # parameter tuning in inner loop
    print('Hyperparameter selection LASSO')
    mdl_enet_inner <- caret::train(X_train, y_train, method = "glmnet",
                                   preProcess = c("center", "scale"),
                                   trControl = ctrl, 
                                   tuneGrid = enet_grid)
  
    # fit model with best parameter settings to training set in outer loop
    mdl_enet_outer <- caret::train(X_train, y_train, method = "glmnet",
                                   preProcess = c("center", "scale"),
                                   tuneGrid = mdl_enet_inner$bestTune)
    
    # evaluate on test set in outer loop 
    y_pred_enet <- predict(mdl_enet_outer, newdata = X_test)
    eval_enet <- defaultSummary(data.frame(obs = y_test, pred = y_pred_enet))
    cor_acc <- cor(y_test, y_pred_enet)
    eval_enet = eval_enet %>% as.data.frame() %>% t() %>% cbind(cor_acc)
    cv_results[[1]] <-  cv_results[[1]] %>% rbind(eval_enet)
    
  
    #### random forest
    # parameter tuning in inner loop
    print('Hyperparameter selection RF')
    mdl_rf_inner <- caret::train(X_train, y_train, data = training, method = "rf",
                                 preProcess = c("center", "scale"),
                                 ntrees = 1000, importance = TRUE,
                                 tuneGrid = rf_grid,
                                 trControl = ctrl)
    
    # fit model with best parameter settings to training set in outer loop
    mdl_rf_outer <- caret::train(X_train, y_train, method = "rf",
                                 preProcess = c("center", "scale"),
                                 ntrees = 1000, importance = TRUE,
                                 tuneGrid = mdl_rf_inner$bestTune)
    
    # evaluate on test set in outer loop 
    y_pred_rf <- predict(mdl_rf_outer, newdata = X_test)
    eval_rf <- defaultSummary(data.frame(obs = y_test, pred = y_pred_rf))
    cor_acc <- cor(y_test, y_pred_rf)
    eval_rf = eval_rf %>% as.data.frame() %>% t() %>% cbind(cor_acc)
    cv_results[[2]] <-  cv_results[[2]] %>% rbind(eval_rf)
    
    
    #### gradient boosting
    # parameter tuning in inner loop
    print('Hyperparameter selection GBM')
    mdl_gbm_inner <- caret::train(X_train, y_train, 
                                  method = "gbm",
                                  preProcess = c("center", "scale"),
                                  trControl = ctrl, 
                                  tuneGrid = gbm_grid, 
                                  verbose = FALSE)
   
    # fit model with best parameter settings to training set in outer loop
    mdl_gbm_outer <- caret::train(X_train, y_train, 
                                  method = "gbm",
                                  preProcess = c("center", "scale"),
                                  tuneGrid = mdl_gbm_inner$bestTune, 
                                  verbose = FALSE)
                        
    # evaluate on test set in outer loop 
    y_pred_gbm <- predict(mdl_gbm_outer, newdata = X_test)
    eval_gbm <- defaultSummary(data.frame(obs = y_test, pred = y_pred_gbm))
    cor_acc <- cor(y_test, y_pred_gbm)
    eval_gbm = eval_gbm %>% as.data.frame() %>% t() %>% cbind(cor_acc)
    cv_results[[3]] <-  cv_results[[3]] %>% rbind(eval_gbm)
    
  }
  
  return(cv_results)

}

```

## Predict SC Scores From Aggregate Process Data
```{r}

sc_feat_all_pred_sc <- sc_feat_all %>% 
  dplyr::select(-c(pc, mr, VKMR, SPM, xPos_range, ray_x_range)) %>% 
  column_to_rownames("id")

cv_results_sc <- nested_cv(data = sc_feat_all_pred_sc, target = 'sc')

cv_results_sc$enet %>% as.data.frame() %>% map_dbl(mean)

```


```{r}
```



## Predict VKMR Scores From Aggregate Process Data
```{r}

sc_feat_all_pred_vkmr <- sc_feat_all %>% 
  dplyr::select(-c(pc, mr, sc, SPM, xPos_range, ray_x_range)) %>% 
  column_to_rownames("id")

cv_results_vkmr <- nested_cv(data = sc_feat_all_pred_vkmr, target = 'VKMR')


```


## Predict SPM Scores From Aggregate Process Data
```{r}

sc_feat_all_pred_sc <- sc_feat_all %>% 
  dplyr::select(-c(pc, mr, sc, VKMR, xPos_range, ray_x_range)) %>% 
  column_to_rownames("id")

sc_feat_all_pred_sc %>% head()

in_train <- createDataPartition(y = sc_feat_all_pred_sc$SPM,
                                p = 0.7, list = FALSE)

training <- sc_feat_all_pred_sc[ in_train, ]
testing  <- sc_feat_all_pred_sc[-in_train, ]


```


### Baseline - Linear Model
```{r}
ctrl <- trainControl(method = "cv", number = 10)

mdl_lm <- caret::train(SPM ~ ., data = training, method = "lm", 
             preProcess = c("center", "scale"), 
             trControl = ctrl)

y_pred_lm_spm <- predict(mdl_lm, newdata = testing)

eval_lm <- defaultSummary(data.frame(obs = testing$SPM, pred = y_pred_lm_spm))
eval_lm

```





## Predict SC Scores From Item-level Process Data (caret)
### Train-test Split
```{r}

#sc_feat_all_item_wide %>% colnames() %>% str_detect('xPos_range')

sc_feat_all_item_wide_pred_sc <- sc_feat_all_item_wide %>% 
  dplyr::select(-c(pc, mr, VKMR, SPM)) %>% 
  column_to_rownames("id")

sc_feat_all_item_wide_pred_sc %>% head()

in_train <- createDataPartition(y = sc_feat_all_item_wide_pred_sc$sc,
                                p = 0.7, list = FALSE)

training <- sc_feat_all_item_wide_pred_sc[ in_train, ]
testing  <- sc_feat_all_item_wide_pred_sc[-in_train, ]

dim(training)
dim(testing)

```
```{r}
ctrl <- trainControl(method = "cv", number = 10)

Grid <- expand.grid(.lambda = seq(.05, 1, length = 10),
                    .alpha = seq(.05, 1, length = 10))

mdl_enet <- caret::train(sc ~ ., data = training, method = "glmnet",
                  preProcess = c("medianImpute", "center", "scale", "nzv"),
                  trControl = ctrl, tuneGrid = Grid)

y_pred_elnet <- predict(mdl_enet, newdata = testing)

eval_enet <- defaultSummary(data.frame(obs = testing$sc, pred = y_pred_elnet))
eval_enet
```



# Try MLR
# Predict SC scores
```{r}
library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(paradox)

sc_feat_all_pred_sc <- sc_feat_all %>% 
  dplyr::select(-c(pc, mr, VKMR, SPM, xPos_range, ray_x_range)) %>% 
  column_to_rownames("id")

sc_feat_all_pred_sc %>% head()

task_sc = tsk(id = 'id', backend = sc_feat_all_pred_sc, target = "sc")
task_sc

lrn = lrn("regr.ranger")
rdesc = makeResampleDesc("CV", iters = 5)
res = resample(learner = lrn, task = task_sc, resampling = rdesc, measure = list(RMSE))

# train/test split
train_set = sample(task_sc$nrow, 0.8 * task_sc$nrow)
test_set = setdiff(seq_len(task_sc$nrow), train_set)

# train the model
lrn$train(task_sc, row_ids = train_set)

# predict data
prediction = lrn$predict(task_sc, row_ids = test_set)

# calculate performance
prediction$score()

measure = msr("regr.rsq")

# automatic resampling
resampling = rsmp("cv", folds = 10)
rr = resample(task = task_sc, lrn, resampling)
rr$resampling
rr$score(measure)

```

## With nested CV
```{r}
library(paradox)

data("Boston")
Boston

# create task with target sc 
task_sc = TaskRegr$new(id = 'predict_sc', backend = Boston, target = "tax")
task_sc

# get overview of learners
mlr_learners

# create learner with rf algorithm 
lrn_rf = lrn("regr.glmnet")
lrn_rf$param_set

# define inner resampling strategy
rsmp_inner = rsmp("cv", folds=5)

# define evaluation metric
measures = msr("regr.rsq")

# define param grid
param_set = paradox::ParamSet$new(
  params = list(paradox::ParamDbl$new("lambda", lower = 0.001, upper = 0.11)))
param_set

# define stop criterion for parameter search
terminator = term("evals", n_evals = 5)
tuner = tnr("grid_search", resolution = 10)

# define parameter tuning strategy
at_rf = AutoTuner$new(lrn_rf, rsmp_inner, measures = measures, tune_ps = 
  param_set, terminator, tuner = tuner)

# define outer resampling strategy
rsmp_outer = rsmp("cv", folds = 5)
rr_rf = mlr3::resample(task = task_sc, learner = at_rf, resampling = rsmp_outer)

rr_rf$resampling

```


```{r}
library("mlr3tuning")
library(mlr3)

task = tsk("iris")
task
learner = lrn("classif.rpart")
learner
resampling = rsmp("holdout")
resampling
measures = msr("classif.ce")
measures
param_set = paradox::ParamSet$new(
  params = list(paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)))
param_set
terminator = term("evals", n_evals = 5)
terminator
tuner = tnr("grid_search", resolution = 10)
tuner
at = AutoTuner$new(learner, resampling, measures = measures,
  param_set, terminator, tuner = tuner)

resampling_outer = rsmp("cv", folds = 3)
rr = resample(task = task, learner = at, resampling = resampling_outer)
```



## Predict VKMR scores 
```{r}

# generate time_taken variable 

sc_feat_all_pred_vkmr <- sc_feat_all %>% dplyr::select(-c(time_taken, pc, mr, sc, SPM)) %>% column_to_rownames("id")
sc_feat_all_pred_vkmr %>% head()

task_vkmr = TaskRegr$new(id = 'id', backend = sc_feat_all_pred_vkmr, target = "VKMR")
task_vkmr

mlr_learners

lrn = lrn("regr.ranger")
rdesc = makeResampleDesc("CV", iters = 5)
res = resample(learner = lrn, task = regr.task, resampling = rdesc, measure = list(RMSE))

# train/test split
train_set = sample(task_vkmr$nrow, 0.8 * task_vkmr$nrow)
test_set = setdiff(seq_len(task_vkmr$nrow), train_set)

# train the model
lrn$train(task_vkmr, row_ids = train_set)

# predict data
prediction = lrn$predict(task_sc, row_ids = test_set)

# calculate performance
prediction$score()

measure = msr("regr.rsq")

# automatic resampling
resampling = rsmp("cv", folds = 10)
rr = resample(task_vkmr, lrn, resampling)
rr$score(measure)
```







```{r}
library("mlr3tuning")
iris


task = tsk(iris)


learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measures = msr("classif.ce")
param_set = paradox::ParamSet$new(params = list(paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)))
terminator = term("evals", n_evals = 5)
tuner = tnr("grid_search", resolution = 10)

at = AutoTuner$new(learner, resampling, measures = measures,
  param_set, terminator, tuner = tuner)
```

